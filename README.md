<!-- Improved compatibility of “Back to top” link -->
<a name="readme-top"></a>

<!-- PROJECT LOGO / BANNER -->
<!-- Replace the src URL or delete completely -->
<p align="center">
  <img src="https://github.com/JNathan18/Banner/blob/main/GBR_Logo2.jpg?raw=true" alt="title_banner">
</p>

<h1 align="center">Detection of Shallow Reef Structures Using Machine Learning</h1>
<p align="center">
  Training a CNN to classify shallow reef structures using SENTINEL-2 data
</p>
<br />

<!-- TABLE OF CONTENTS (clickable) -->
<!-- TABLE OF CONTENTS (clickable) -->
<details>
  <summary>Table of Contents</summary>
  <ol>
    <li>
      <a href="#about-the-project">About The Project</a>
    </li>
    <li>
      <a href="#background">Background</a>
    </li>
    <li>
      <a href="#sentinel-2">Sentinel-2</a>
      <ul>
        <li><a href="#mission-at-a-glance">Mission at a glance</a></li>
        <li><a href="#why-only-blue-green-red--nir-for-this-project">Why only Blue, Green, Red &amp; NIR for this project</a></li>
      </ul>
    </li>
    <li>
      <a href="#convolutional-neural-networks-cnns">Convolutional Neural Networks (CNNs)</a>
      <ul>
        <li><a href="#what-are-they">What are they?</a></li>
        <li><a href="#why-we-chose-a-cnn-for-reef-mapping">Why we chose a CNN for reef mapping</a></li>
      </ul>
    </li>
    <li>
      <a href="#ndwi">NDWI</a>
      <ul>
        <li><a href="#what-is-it">What is it?</a></li>
        <li><a href="#why-include-ndwi-in-our-pipeline">Why include NDWI in our pipeline?</a></li>
      </ul>
    </li>
    <li>
      <a href="#end-to-end-workflow-at-a-glance">End-to-End Workflow at a Glance</a>
    </li>
    <li>
      <a href="#video-tutorial-on-the-code">Video Tutorial on the Code</a>
    </li>
    <li>
      <a href="#getting-started">Getting Started</a>
    </li>
    <li>
      <a href="#license">License</a>
    </li>
    <li>
      <a href="#contact">Contact</a>
    </li>
    <li>
      <a href="#acknowledgements">Acknowledgements</a>
    </li>
    <li>
      <a href="#refrences">Refrences</a>
    </li>
  </ol>
</details>

<!-- ABOUT THE PROJECT -->
## About The Project

This project was developed as the final assignment for the GEOL0069 (AI for Earth Observation) module at University College London. The module equips students with the skills to apply machine learning (ML) techniques to the growing volume of data generated by Earth observation satellites.

In this project, we apply supervised learning to extract a range of shallow reef features from satellite imagery. We focus on data from Sentinel-2, chosen for its high-resolution multispectral capabilities; an essential requirement for our algorithm of choice, the Convolutional Neural Network (CNN). The aim is to demonstrate how modern ML tools can support ecological monitoring by providing a minimal-footprint workflow that anyone can run quickly on free Sentinel-2 imagery to generate habitat predictions from scratch with only a handful of hand-labelled pixels. To this end, we report emissions due to training using the CodeCarbon package in python (CodeCarbon, 2025).

<p align="right">(<a href="#readme-top">back to top</a>)</p>

## Background

Shallow reef zones are among the most biodiverse ecosystems on Earth, supporting nearly 25% of all marine species (Coral Reef Alliance, n.d.). These structures play vital roles in coastal protection, fisheries, and global biodiversity. However, they are increasingly under threat from climate change, coral bleaching, pollution, and destructive human activity.

Accurately classifying these features using satellite imagery and machine learning allows us to monitor reef health at scale, detect early signs of degradation, and support conservation planning (Phys.org, 2025; Huang et al., 2022). This is essential for protecting ecosystems that are both ecologically rich and critically endangered.

Traditional benthic surveys cannot keep pace with events that unfold on the scale of weeks, and even the best global products refresh too slowly for crisis response. For example, record-breaking water temperatures in the Florida Keys in July 2023 triggered widespread bleaching, yet the first high-resolution satellite comparison that managers could view was not published by NASA Earth Observatory until 8 August 2023, weeks after colonies had already begun to die (Hansen 2023). 

During the 2016 mass-bleaching of the Great Barrier Reef, the initial reef-wide aerial surveys finished in early June 2016, but a second round to assess survivorship did not start until mid-September, a six-month gap when many corals either recovered or perished unseen (Great Barrier Reef Marine Park Authority 2017). 

As a proof-of-concept, our pipeline shows that with just two co-registered tiles it is possible to generate baseline coral-cover maps for an entire reef tract within hours rather than weeks. While the current code does not yet quantify storm or bleaching damage, the workflow demonstrates a lightweight, emission-friendly and easily ported method that could give managers a timely snapshot to prioritise where to send divers, close fisheries, or protect nurseries.

<p align="right">(<a href="#readme-top">back to top</a>)</p>

## Sentinel-2 

### Mission at a glance  
Sentinel-2 is part of ESA’s Copernicus fleet and comprises the twin satellites Sentinel-2A (2015) and Sentinel-2B (2017) (ESA, 2024).  
They share a sun-synchronous 786 km orbit, phased 180 ° apart (Copernicus Data Space Ecosystem, n.d.), giving:

| Parameter | Value | Why it matters for reefs |
|-----------|-------|--------------------------|
| **Revisit time** | 10 days per sat, 5 days combined (equator) | Frequent looks capture bleaching events and seasonal change |
| **Swath width** | 290 km | One pass covers entire archipelagos |

The Sentinel-2 Multi-Spectral Instrument (MSI) is a push-broom camera: as the satellite advances, a single linear detector sweeps out a continuous image strip. Incoming light is folded by three mirrors and split by a beam-splitter onto two focal-plane assemblies (FPAs), one covering the visible to near-infra-red range and the other the short-wave infra-red. Across these FPAs, 13 strip filters carve out the discrete spectral bands used by Sentinel-2: four bands are captured at 10 m ground resolution (blue, green, red, NIR), six at 20 m, and three at 60 m (NASA Earthdata, n.d.).

![Sentinel-2 multi-band imaging overview](https://github.com/JNathan18/Banner/blob/main/image.png)

### Why only Blue, Green, Red & NIR for this project  
1. All bands are at 10 m which keeps spatial detail consistent.  
2. Blue/Green penetrate the water column; Red enhances benthic discrimination. 
3. Just four channels means smaller tensors which means faster training & real-time inference on edge devices.
4. NIR enables computation of NDWI masks for our regions.

<p align="right">(<a href="#readme-top">back to top</a>)</p>

## Convolutional Neural Networks (CNNs)

### What are they?

A Convolutional Neural Network is the deep-learning work-horse for image recognition. Conceptually, it mimics the way our visual cortex processes information: small receptive fields scan across the scene, detect primitive patterns (edges, corners, colour blobs), and pass progressively richer abstractions to later layers that decide what the image contains (O'Shea and Nash, 2015).

**How it works**

1. **Feature-extraction stage:** Every convolutional layer slides a tiny filter (11 × 11 pixels in our case) over the input, multiplying and summing to produce a feature map. Non-linear activations (ReLU) follow (Olamendy, 2023), and an occasional pooling step downsamples the map, keeping only the most salient responses (Encord, 2023).  Because the same weights are reused everywhere (weight sharing), the network is compact and learns position-invariant patterns.

2. **Classification stage:** Once enough feature maps have been stacked, the 3-D tensor is flattened and fed to one or more fully-connected layers.  These dense neurons mix the extracted cues and output a probability for each class via a soft-max function.

<div align="center">

![Convolutional Neural Network architecture](https://github.com/JNathan18/Banner/blob/main/Sen_2_Infographic_cnn.png)

</div>

### Why we chose a CNN for reef mapping

Convolutions inspect only a handful of pixels at a time, perfect for teasing apart benthic textures (live coral, algae, sand) that occupy just a few Sentinel-2 pixels. With only the four 10 m bands (Blue, Green, Red, NIR) as input, a shallow CNN compiles to < 1 MB when quantised.

<p align="right">(<a href="#readme-top">back to top</a>)</p>

## NDWI  
### What is it?
The Normalised Difference Water Index (NDWI) was introduced by (McFeeters, 1996) as a simple way to highlight open water using only two spectral bands:

$$
\text{NDWI} = \frac{\text{Green} - \text{NIR}}{\text{Green} + \text{NIR}}
$$

* **Sentinel-2 band mapping** — Green = B3 (560 nm); NIR = B8 (842 nm).  
* **Value range** — NDWI ∈ [-1, +1].  Positive values tend to mark water; negative values mark vegetation, bare land, or coral substrate.

### Why include NDWI in our pipeline?  
 
A lightweight CNN like ours may confuse deep ocean noise for genuine reef signals. By introducing an NDWI filter, we reduce spurious detections by masking out areas unlikely to contain water (Li et al., 2022). The index is also invariant to overall brightness (ratio form), so it supplies contrast information that the four raw Sentinel-2 bands (Blue, Green, Red, NIR) do not capture on their own (Xu, 2006). This gives the CNN crucial context for distinguishing where ‘wet’ ends and ‘dry’ begins. On top of this it is lightweight on compute and can be performed as a one line operation on-the-fly during data loading.

<p align="right">(<a href="#readme-top">back to top</a>)</p>

## End-to-End Workflow at a Glance

1. **Select cloud-free Sentinel-2 tiles**  
   Query neighbouring tiles acquired on the same date to guarantee spectral consistency.

2. **Compute an NDWI filter**  
   An NDWI layer will also be added as a 5ᵗʰ channel to every 11 × 11 patch.

3. **Define Regions of Interest (ROIs)**  
   Draw polygons on multiple reference tiles covering reef flat, shallow sand, etc.

4. **Label the ROIs with IRIS**  
   Intelligently Reinforced Image Segmentation (IRIS) software auto-classifies polygons and exports pixel-wise masks.

5. **Build a balanced training set**  
   Sample equal numbers of patches per class.

6. **Train the CNN**  
   Two-block ConvNet, Early-Stopping, ReduceLR; NDWI included as an extra input channel. CodeCarbon logs CO₂, kWh, CPU/GPU power.

7. **Roll-out on the unseen tile**  
   Predict on a second, previously untouched tile; NDWI recomputed on-the-fly.

8. **Evaluate & analyse**  
   This includes a Confusion matrix as well as f1-scores, precision, recall and accuracy for our classes, overlay of our classification against the ACA as the ground truth. Here, we 
   also discuss the environmental impact of training such a model.

<p align="right">(<a href="#readme-top">back to top</a>)</p>

## Video Tutorial on the Code

[![Watch the walkthrough](https://img.youtube.com/vi/d3tk6PJQwNU/0.jpg)](https://www.youtube.com/watch?v=d3tk6PJQwNU)

<p align="right">(<a href="#readme-top">back to top</a>)</p>

## Getting Started

Developed in [Google Colab](https://colab.research.google.com/), this project leverages cloud-based Python execution for streamlined development and sharing. The following steps will guide you through reproducing or adapting the workflow.

1. **Download the notebook**  
   Get a copy of `CNN Reef Classifier.ipynb` from this repository.

2. **Install prerequisite libraries**
  ```bash
  !pip -q install requests shapely rasterio tqdm fiona codecarbon
  ```

3. **Acquire the required datasets**  
   - **Sentinel-2 imagery**: This is downloaded locally within our notebook. Tiles are taken from regions around the great barrier reef. A Copernicus account will be needed for data 
   collection. It is free to set up and once you have verified your account, you may proceed with the download. Input your credentials when prompted in the script.
   - **Ground truth labels**: We use benthic habitat maps from the [Allen Coral Atlas (ACA)](https://allencoralatlas.org/atlas/#4.57/-16.5163/147.1100). Go to the link, select the relevant region (e.g., Torres Strait), download the dataset, extract the files, and note the path to the `.gpkg` or raster layers.

4. **Update file paths in the notebook**  
   Modify any file paths to match where you’ve saved your datasets:
   - Update paths to Sentinel-2 `.npy` or `.jp2` files.
   - Set the path to the ACA mask or shapefile used for evaluation.

   If you are using the same data as the original notebook, minimal changes should be needed, just download the masks provided in the repo and provide paths to them.
   If using your own data, remember to adjust file names and region-of-interest definitions accordingly, I have provided `config.json` so that you can download and edit to create your 
   own training data within IRIS.

5. **Run the notebook cells in order**  
   Each cell walks you through preprocessing, training the CNN, applying the NDWI filter, evaluating predictions, and generating outputs.  
   Inline comments provide further guidance throughout.

<p align="right">(<a href="#readme-top">back to top</a>)</p>

## License

Distributed under the MIT License. See `LICENSE.txt` for more information.

<p align="right">(<a href="#readme-top">back to top</a>)</p>

## Contact
Jonathan McLennan - [jonomclennan@gmail.com](mailto:jonomclennan@gmail.com) / [zcapmcl@ucl.ac.uk](mailto:zcapmcl@ucl.ac.uk) / [https://www.linkedin.com/in/jonathan-mclennan](https://www.linkedin.com/in/jonathan-mclennan-172039236/)

Project Link: https://github.com/JNathan18/Detection-of-Shallow-Reef-Structures

<p align="right">(<a href="#readme-top">back to top</a>)</p>

## Acknowledgements

This project was created for GEOL0069 at University College London. This module taught by professor Dr. Michel Tsamados and Weibin Chen.

<p align="right">(<a href="#readme-top">back to top</a>)</p>

## References

Hansen, K. (2023) ‘Confronting Florida’s Coral Collapse’. NASA Earth Observatory, 8 August. Available at: https://earthobservatory.nasa.gov/images/151692/confronting-floridas-coral-collapse (Accessed: 2 June 2025).

Great Barrier Reef Marine Park Authority (GBRMPA) (2017) Final report: 2016 coral bleaching event on the Great Barrier Reef. Townsville: GBRMPA. Available at: https://www.gbrmpa.gov.au/sites/default/files/2016%20Coral%20Bleaching%20Event%20Final%20Assessment%20Report.pdf (Accessed: 2 June 2025).

CodeCarbon (2025) CodeCarbon: Estimate and Track Carbon Emissions from Machine Learning Computing. Available at: https://pypi.org/project/codecarbon/ (Accessed: 29 May 2025).

Allen Coral Atlas (n.d.) *Mapping Methods*. Available at: https://allencoralatlas.org/methods/ (Accessed: 2 June 2025).

Coral Reef Alliance (n.d.) *Why Care About Reefs: Biodiversity*. Available at: https://coral.org/en/coral-reefs-101/why-care-about-reefs/biodiversity/ (Accessed: 29 May 2025).

Huang, H., Asner, G. P., Vaughn, N., Knapp, D. E., & Martin, R. E. (2022) ‘Automated Coral Reef Classification with Remote Sensing and Machine Learning’, *Remote Sensing*, 14(11), p.2666. doi:10.3390/rs14112666.

NOAA (2024) *Coral Reefs and Climate Change*. Available at: https://oceanservice.noaa.gov/facts/coralreef-climate.html (Accessed: 30 May 2025).

Phys.org (2025) *Could satellites be the solution to tracking coral reef health?* Available at: https://phys.org/news/2025-04-satellites-solution-tracking-coral-reef.html (Accessed: 31 May 2025).

Copernicus Data Space Ecosystem (n.d.) *Sentinel-2*. Available at: https://dataspace.copernicus.eu/explore-data/data-collections/sentinel-data/sentinel-2 (Accessed: 30 May 2025).

NASA Earthdata (n.d.) *Sentinel-2 MSI*. Available at: https://www.earthdata.nasa.gov/data/instruments/sentinel-2-msi (Accessed: 1 June 2025).

ESA (2024) *Sentinel-2: The Operational Copernicus Optical High Resolution Land Mission*. Available at: https://www.esa.int/Applications/Observing_the_Earth/Copernicus/Sentinel-2 (Accessed: 1 June 2025).

Encord (2023) *Convolutional Neural Networks (CNN) Overview*. Available at: https://encord.com/blog/convolutional-neural-networks-explained/ (Accessed: 2 June 2025).

Olamendy, J.C. (2023) 'Back to Basics: Feature Extraction with CNN', *Medium*, 20 October. Available at: https://medium.com/@juanc.olamendy/back-to-basics-feature-extraction-with-cnn-16b2d405011a (Accessed: 2 June 2025).

O'Shea, K. and Nash, R. (2015) 'An Introduction to Convolutional Neural Networks', *arXiv preprint arXiv:1511.08458*. Available at: https://arxiv.org/abs/1511.08458 (Accessed: 2 June 2025).

Li, J., Wang, J., Li, Y., and Liu, D. (2022) ‘An Improved NDWI-Based Method for Surface Water Extraction Using Sentinel-2 Imagery’, Remote Sensing, 14(2), p. 292.
Available at: https://doi.org/10.3390/rs14020292 (Accessed: 2 June 2025).

McFeeters, S.K. (1996) ‘The use of the Normalized Difference Water Index (NDWI) in the delineation of open water features’, International Journal of Remote Sensing, 17(7), pp. 1425–1432.
Available at: https://doi.org/10.1080/01431169608948714 (Accessed: 2 June 2025).

Xu, H. (2006) ‘Modification of Normalised Difference Water Index (NDWI) to enhance open water features in remotely sensed imagery’, International Journal of Remote Sensing, 27(14), pp. 3025–3033.
Available at: https://doi.org/10.1080/01431160600589179 (Accessed: 2 June 2025).

<p align="right">(<a href="#readme-top">back to top</a>)</p>
