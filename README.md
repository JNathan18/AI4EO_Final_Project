<!-- Improved compatibility of “Back to top” link -->
<a name="readme-top"></a>

<!-- PROJECT LOGO / BANNER -->
<!-- Replace the src URL or delete completely -->
<p align="center">
  <img src="https://github.com/JNathan18/Banner/blob/main/GBR_Logo2.jpg?raw=true" alt="title_banner">
</p>

<h1 align="center">Detection of Shallow Reef Structures Using Machine Learning</h1>
<p align="center">
  Training a CNN to classify shallow reef structures using SENTINEL-2 data
</p>
<br />

<!-- TABLE OF CONTENTS (clickable) -->
<!-- TABLE OF CONTENTS (clickable) -->
<details>
  <summary>Table of Contents</summary>
  <ol>
    <li>
      <a href="#about-the-project">About The Project</a>
    </li>
    <li>
      <a href="#background">Background</a>
    </li>
    <li>
      <a href="#sentinel-2">Sentinel-2</a>
      <ul>
        <li><a href="#mission-at-a-glance">Mission at a glance</a></li>
        <li><a href="#why-only-blue-green-red--nir-for-this-project">Why only Blue, Green, Red &amp; NIR for this project</a></li>
      </ul>
    </li>
    <li>
      <a href="#convolutional-neural-networks-cnns">Convolutional Neural Networks (CNNs)</a>
      <ul>
        <li><a href="#what-are-they">What are they?</a></li>
        <li><a href="#why-we-chose-a-cnn-for-reef-mapping">Why we chose a CNN for reef mapping</a></li>
      </ul>
    </li>
    <li>
      <a href="#ndwi">NDWI</a>
      <ul>
        <li><a href="#what-is-it">What is it?</a></li>
        <li><a href="#why-include-ndwi-in-our-pipeline">Why include NDWI in our pipeline?</a></li>
      </ul>
    </li>
    <li>
      <a href="#end-to-end-workflow-at-a-glance">End-to-End Workflow at a Glance</a>
    </li>
    <li>
      <a href="#getting-started">Getting Started</a>
    </li>
    <li>
      <a href="#license">License</a>
    </li>
    <li>
      <a href="#contact">Contact</a>
    </li>
    <li>
      <a href="#acknowledgements">Acknowledgements</a>
    </li>
    <li>
      <a href="#refrences">Refrences</a>
    </li>
  </ol>
</details>

<!-- ABOUT THE PROJECT -->
## About The Project

This project was developed as the final assignment for the GEOL0069 (AI for Earth Observation) module at University College London. The module equips students with the skills to apply machine learning (ML) techniques to the growing volume of data generated by Earth observation satellites.

In this project, we apply supervised learning to extract a range of shallow reef features from satellite imagery. We focus on data from Sentinel-2, chosen for its high-resolution multispectral capabilities; an essential requirement for our algorithm of choice, the Convolutional Neural Network (CNN). The aim is to demonstrate how modern ML tools can support ecological monitoring by providing a minimal-footprint workflow that anyone can run on free Sentinel-2 imagery to generate habitat predictions from scratch with only a handful of hand-labelled pixels. To this end, we report emissions due to training using the CodeCarbon package in python.

<p align="right">(<a href="#readme-top">back to top</a>)</p>

## Background

Shallow reef zones are among the most biodiverse ecosystems on Earth, supporting nearly 25% of all marine species. These structures play vital roles in coastal protection, fisheries, and global biodiversity. However, they are increasingly under threat from climate change, coral bleaching, pollution, and destructive human activity.

Accurately classifying these features using satellite imagery and machine learning allows us to monitor reef health at scale, detect early signs of degradation, and support conservation planning. This is essential for protecting ecosystems that are both ecologically rich and critically endangered.

Traditional benthic surveys cannot keep pace across thousands of dispersed reefs, and global mapping products like the Allen Coral Atlas, although invaluable, are computationally heavy and rely on extensive training data that may not exist for every site. Because our pipeline needs just two co-registered tiles, the method can be ported along a reef tract in hours, supporting near-real-time monitoring after storms or heatwaves.

<p align="right">(<a href="#readme-top">back to top</a>)</p>

## Sentinel-2 

### Mission at a glance  
Sentinel-2 is part of ESA’s Copernicus fleet and comprises the twin satellites Sentinel-2A (2015) and Sentinel-2B (2017).  
They share a sun-synchronous 786 km orbit, phased 180 ° apart, giving:

| Parameter | Value | Why it matters for reefs |
|-----------|-------|--------------------------|
| **Revisit time** | 10 days per sat, 5 days combined (equator) | Frequent looks capture bleaching events and seasonal change |
| **Swath width** | 290 km | One pass covers entire archipelagos |

The Sentinel-2 Multi-Spectral Instrument (MSI) is a push-broom camera: as the satellite advances, a single linear detector sweeps out a continuous image strip. Incoming light is folded by three mirrors and split by a beam-splitter onto two focal-plane assemblies (FPAs), one covering the visible to near-infra-red range and the other the short-wave infra-red. Across these FPAs, 13 strip filters carve out the discrete spectral bands used by Sentinel-2: four bands are captured at 10 m ground resolution (blue, green, red, NIR), six at 20 m, and three at 60 m.

![Sentinel-2 multi-band imaging overview](https://github.com/JNathan18/Banner/blob/main/image.png)

### Why only Blue, Green, Red & NIR for this project  
1. All bands are at 10 m which keeps spatial detail consistent.  
2. Blue/Green penetrate the water column; Red enhances benthic discrimination. 
3. Just four channels means smaller tensors which means faster training & real-time inference on edge devices.
4. NIR enables computation of NDWI masks for our regions.

<p align="right">(<a href="#readme-top">back to top</a>)</p>

## Convolutional Neural Networks (CNNs)

### What are they?

A Convolutional Neural Network is the deep-learning work-horse for image recognition. Conceptually, it mimics the way our visual cortex processes information: small receptive fields scan across the scene, detect primitive patterns (edges, corners, colour blobs), and pass progressively richer abstractions to later layers that decide what the image contains.

**How it works**

1. **Feature-extraction stage:** Every convolutional layer slides a tiny filter (11 × 11 pixels in our case) over the input, multiplying and summing to produce a feature map. Non-linear activations (ReLU) follow, and an occasional pooling step downsamples the map, keeping only the most salient responses.  Because the same weights are reused everywhere (weight sharing), the network is compact and learns position-invariant patterns.

2. **Classification stage:** Once enough feature maps have been stacked, the 3-D tensor is flattened and fed to one or more fully-connected layers.  These dense neurons mix the extracted cues and output a probability for each class via a soft-max function.

<div align="center">

![Convolutional Neural Network architecture](https://github.com/JNathan18/Banner/blob/main/Sen_2_Infographic_cnn.png)

</div>

### Why we chose a CNN for reef mapping

Convolutions inspect only a handful of pixels at a time, perfect for teasing apart benthic textures (live coral, algae, sand) that occupy just a few Sentinel-2 pixels. With only the four 10 m bands (Blue, Green, Red, NIR) as input, a shallow CNN compiles to < 1 MB when quantised.

<p align="right">(<a href="#readme-top">back to top</a>)</p>

## NDWI  
### What is it?
The Normalised Difference Water Index (NDWI) was introduced by McFeeters (1996) as a simple way to highlight open water using only two spectral bands:

$$
\text{NDWI} = \frac{\text{Green} - \text{NIR}}{\text{Green} + \text{NIR}}
$$

* **Sentinel-2 band mapping** — Green = B3 (560 nm); NIR = B8 (842 nm).  
* **Value range** — NDWI ∈ [-1, +1].  Positive values tend to mark water; negative values mark vegetation, bare land, or coral substrate.

### Why include NDWI in our pipeline?  
 
A lightweight CNN like ours may confuse deep ocean noise for genuine reef signals. By introducing an NDWI filter, we reduce spurious detections by masking out areas unlikely to contain water. The index is also invariant to overall brightness (ratio form), so it supplies contrast information that the four raw Sentinel-2 bands (Blue, Green, Red, NIR) do not capture on their own. This gives the CNN crucial context for distinguishing where ‘wet’ ends and ‘dry’ begins. On top of this it is lightweight on compute and can be performed as a one line operation on-the-fly during data loading.

<p align="right">(<a href="#readme-top">back to top</a>)</p>

## End-to-End Workflow at a Glance

1. **Select cloud-free Sentinel-2 tiles**  
   Query neighbouring tiles acquired on the same date to guarantee spectral consistency.

2. **Compute an NDWI filter**  
   An NDWI layer will also be added as a 5ᵗʰ channel to every 11 × 11 patch.

3. **Define Regions of Interest (ROIs)**  
   Draw polygons on multiple reference tiles covering reef flat, shallow sand, etc.

4. **Label the ROIs with IRIS**  
   Intelligently Reinforced Image Segmentation (IRIS) software auto-classifies polygons and exports pixel-wise masks.

5. **Build a balanced training set**  
   Sample equal numbers of patches per class.

6. **Train the CNN**  
   Two-block ConvNet, Early-Stopping, ReduceLR; NDWI included as an extra input channel. CodeCarbon logs CO₂, kWh, CPU/GPU power.

7. **Roll-out on the unseen tile**  
   Predict on a second, previously untouched tile; NDWI recomputed on-the-fly.

8. **Evaluate & analyse**  
   This includes a Confusion matrix as well as f1-scores, precision, recall and accuracy for our classes, overlay of our classification against the ACA as the ground truth.

<p align="right">(<a href="#readme-top">back to top</a>)</p>

## Getting Started

Developed in [Google Colab](https://colab.research.google.com/), this project leverages cloud-based Python execution for streamlined development and sharing. The following steps will guide you through reproducing or adapting the workflow.

1. **Download the notebook**  
   Get a copy of `CNN Reef Classifier.ipynb` from this repository.

2. **Install prerequisite libraries**
  ```bash
  !pip -q install requests shapely rasterio tqdm fiona codecarbon
  ```

3. **Acquire the required datasets**  
   - **Sentinel-2 imagery**: This is downloaded locally within our notebook. Tiles are taken from regions around the great barrier reef. A Copernicus account will be needed for data 
   collection. It is free to set up and once you have verified your account, you may proceed with the download. Input your credentials when prompted in the script.
   - **Ground truth labels**: We use benthic habitat maps from the [Allen Coral Atlas (ACA)](https://allencoralatlas.org/atlas/#4.57/-16.5163/147.1100).  
     - Go to the link, select the relevant region (e.g., Torres Strait), download the dataset, extract the files, and note the path to the `.gpkg` or raster layers.

4. **Update file paths in the notebook**  
   Modify any file paths to match where you’ve saved your datasets:
   - Update paths to Sentinel-2 `.npy` or `.jp2` files.
   - Set the path to the ACA mask or shapefile used for evaluation.

   If you are using the same data as the original notebook, minimal changes should be needed.  
   If using your own data, remember to adjust file names and region-of-interest definitions accordingly.

5. **Run the notebook cells in order**  
   Each cell walks you through preprocessing, training the CNN, applying the NDWI filter, evaluating predictions, and generating outputs.  
   Inline comments provide further guidance throughout.

<p align="right">(<a href="#readme-top">back to top</a>)</p>

## License

Distributed under the MIT License. See `LICENSE.txt` for more information.

<p align="right">(<a href="#readme-top">back to top</a>)</p>

## Contact
Jonathan McLennan - [jonomclennan@gmail.com](mailto:jonomclennan@gmail.com) / [zcapmcl@ucl.ac.uk](mailto:zcapmcl@ucl.ac.uk) / [https://www.linkedin.com/in/jonathan-mclennan](https://www.linkedin.com/in/jonathan-mclennan-172039236/)

Project Link: 

<p align="right">(<a href="#readme-top">back to top</a>)</p>

## Acknowledgements

This project was created for GEOL0069 at University College London. This module taught by professor Dr. Michel Tsamados and Weibin Chen.

<p align="right">(<a href="#readme-top">back to top</a>)</p>

## Refrences
