<!-- Improved compatibility of “Back to top” link -->
<a name="readme-top"></a>

<!-- PROJECT LOGO / BANNER -->
<!-- Replace the src URL or delete completely -->
<p align="center">
  <img src="https://github.com/JNathan18/Banner/blob/main/GBR_Logo2.jpg?raw=true" alt="title_banner">
</p>

<h1 align="center">Detection of Shallow Reef Structures Using Machine Learning</h1>
<p align="center">
  Training a CNN to classify shallow reef structures using SENTINEL-2 data
</p>
<br />

<!-- TABLE OF CONTENTS (clickable) -->
<details>
  <summary>Table of Contents</summary>
  <ol>
    <li>
      <a href="#about-the-project">About&nbsp;The&nbsp;Project</a>
      <ul>
        <li><a href="#background">Background</a></li>
        <li><a href="#key-concepts">Key Concepts</a></li>
        <li><a href="#methodology">Methodology</a></li>
      </ul>
    </li>
    <li>
      <a href="#getting-started">Getting Started</a>
      <ul>
        <li><a href="#prerequisites">Prerequisites</a></li>
        <li><a href="#datasets-or-inputs">Datasets&nbsp;or&nbsp;Inputs</a></li>
      </ul>
    </li>
    <li><a href="#usage">Usage</a></li>
    <li><a href="#license">License</a></li>
    <li><a href="#contact">Contact</a></li>
    <li>
      <a href="#acknowledgments">Acknowledgments</a>
      <ul>
        <li><a href="#references">References</a></li>
      </ul>
    </li>
  </ol>
</details>

<!-- ABOUT THE PROJECT -->
## About The Project

This project was developed as the final assignment for the GEOL0069 (AI for Earth Observation) module at University College London. The module equips students with the skills to apply machine learning (ML) techniques to the growing volume of data generated by Earth observation satellites.

In this project, we apply supervised learning to extract a range of shallow reef features from satellite imagery. We focus on data from Sentinel-2, chosen for its high-resolution multispectral capabilities; an essential requirement for our algorithm of choice, the Convolutional Neural Network (CNN). The aim is to demonstrate how modern ML tools can support ecological monitoring by providing a minimal-footprint workflow that anyone can run on free Sentinel-2 imagery to generate habitat predictions from scratch with only a handful of hand-labelled pixels.

## Background

Shallow reef zones are among the most biodiverse ecosystems on Earth, supporting nearly 25% of all marine species. These structures play vital roles in coastal protection, fisheries, and global biodiversity. However, they are increasingly under threat from climate change, coral bleaching, pollution, and destructive human activity.

Accurately classifying these features using satellite imagery and machine learning allows us to monitor reef health at scale, detect early signs of degradation, and support conservation planning. This is essential for protecting ecosystems that are both ecologically rich and critically endangered.

Traditional benthic surveys cannot keep pace across thousands of dispersed reefs, and global mapping products like the Allen Coral Atlas, although invaluable, are computationally heavy and rely on extensive training data that may not exist for every site. Because our pipeline needs just two co-registered tiles, the method can be ported along a reef tract in hours, supporting near-real-time monitoring after storms or heatwaves.

## Sentinel-2 

### Mission at a glance  
Sentinel-2 is part of ESA’s **Copernicus** fleet and comprises the twin satellites **Sentinel-2A** (2015) and **Sentinel-2B** (2017).  
They share a sun-synchronous 786 km orbit, phased 180 ° apart, giving:

| Parameter | Value | Why it matters for reefs |
|-----------|-------|--------------------------|
| **Revisit time** | 10 days per sat, **5 days combined** (equator) | Frequent looks capture bleaching events and seasonal change |
| **Swath width** | 290 km | One pass covers entire archipelagos |
| **Local overpass** | ~10 : 30 a.m. LTAN | Consistent illumination → easier atmospheric correction |

The Sentinel-2 Multi-Spectral Instrument (MSI) is a push-broom camera: as the satellite advances, a single linear detector sweeps out a continuous image strip. Incoming light is folded by three mirrors and split by a beam-splitter onto two focal-plane assemblies—one covering the visible to near-infra-red range (VNIR, 0.43–0.95 µm) and the other the short-wave infra-red (SWIR, 1.37–2.19 µm). Across these FPAs, 13 strip filters carve out the discrete spectral bands used by Sentinel-2: four bands are captured at 10 m ground resolution (blue, green, red, NIR), six at 20 m, and three at 60 m.

![Sentinel-2 multi-band imaging overview](https://github.com/JNathan18/Banner/blob/main/image.png)

### Why only Blue, Green, Red & NIR for this project  
1. **All at 10 m** → keeps spatial detail consistent.  
2. Blue/Green penetrate the water column; Red enhances benthic discrimination; NIR gives razor-sharp land/water masks.  
3. Four channels = smaller tensors → faster training & real-time inference on edge devices.
4. Enables computation of NDWI masks for our regions

## Convolutional Neural Networks (CNNs)

A **Convolutional Neural Network** is the deep-learning work-horse for image recognition.  
Conceptually, it mimics the way our visual cortex processes information: small receptive fields
scan across the scene, detect primitive patterns (edges, corners, colour blobs), and pass
progressively richer abstractions to later layers that decide *what* the image contains.

**How it works**

1. **Feature-extraction stage** — Every convolutional layer slides a tiny filter  
   (11 × 11 pixels in our case) over the input, multiplying and summing to produce a *feature map*.  
   Non-linear activations (ReLU) follow, and an occasional *pooling* step downsamples the map,
   keeping only the most salient responses.  Because the same weights are reused everywhere
   (*weight sharing*), the network is compact and learns position-invariant patterns.

2. **Classification stage** — Once enough feature maps have been stacked, the 3-D tensor is
   flattened and fed to one or more fully-connected layers.  These dense neurons mix the
   extracted cues and output a probability for each class via a soft-max function.

<div align="center">

![Convolutional Neural Network architecture](https://github.com/JNathan18/Banner/blob/main/Sen_2_Infographic_cnn.png)

</div>

### Why we chose a CNN for reef mapping

* **Locality at 10 m** – Convolutions inspect only a handful of pixels at a time, perfect for
  teasing apart benthic textures (live coral, algae, sand) that occupy just a few Sentinel-2
  pixels.

* **Model economy** – With only the four 10 m bands (Blue, Green, Red, NIR) as input, a shallow
  CNN compiles to < 1 MB when quantised.  That means real-time inference on edge devices
  (drones, Raspberry Pi) without a GPU.

* **Explainability hooks** – Gradient-based saliency maps and Grad-CAM can be applied directly to
  the convolutional feature maps, letting us *see* which reef patches the network relied on and
  verify that its decisions are geologically sensible.


### Methodology
[Describe your overall approach or algorithm.  
Bullet points or sub-headings (“Data Pre-processing”, “Model”, “Evaluation”) often read well here.]

<p align="right">(<a href="#readme-top">back to top</a>)</p>

## Getting Started

### Prerequisites
* List software versions, libraries, or accounts needed  
  ```bash
  pip install -r requirements.txt


