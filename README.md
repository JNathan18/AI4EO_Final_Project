<!-- Improved compatibility of “Back to top” link -->
<a name="readme-top"></a>

<!-- PROJECT LOGO / BANNER -->
<!-- Replace the src URL or delete completely -->
<p align="center">
  <img src="https://github.com/JNathan18/Banner/blob/main/GBR_Logo2.jpg?raw=true" alt="title_banner">
</p>

<h1 align="center">Detection of Shallow Reef Structures Using Machine Learning</h1>
<p align="center">
  Training a CNN to classify shallow reef structures using SENTINEL-2 data
</p>
<br />

<!-- TABLE OF CONTENTS (clickable) -->
<details>
  <summary>Table of Contents</summary>
  <ol>
    <li>
      <a href="#about-the-project">About&nbsp;The&nbsp;Project</a>
      <ul>
        <li><a href="#background">Background</a></li>
        <li><a href="#key-concepts">Key Concepts</a></li>
        <li><a href="#methodology">Methodology</a></li>
      </ul>
    </li>
    <li>
      <a href="#getting-started">Getting Started</a>
      <ul>
        <li><a href="#prerequisites">Prerequisites</a></li>
        <li><a href="#datasets-or-inputs">Datasets&nbsp;or&nbsp;Inputs</a></li>
      </ul>
    </li>
    <li><a href="#usage">Usage</a></li>
    <li><a href="#license">License</a></li>
    <li><a href="#contact">Contact</a></li>
    <li>
      <a href="#acknowledgments">Acknowledgments</a>
      <ul>
        <li><a href="#references">References</a></li>
      </ul>
    </li>
  </ol>
</details>

<!-- ABOUT THE PROJECT -->
## About The Project

This project was developed as the final assignment for the GEOL0069 (AI for Earth Observation) module at University College London. The module equips students with the skills to apply machine learning (ML) techniques to the growing volume of data generated by Earth observation satellites.

In this project, we apply supervised learning to extract a range of shallow reef features from satellite imagery. We focus on data from Sentinel-2, chosen for its high-resolution multispectral capabilities; an essential requirement for our algorithm of choice, the Convolutional Neural Network (CNN). The aim is to demonstrate how modern ML tools can support ecological monitoring by providing a minimal-footprint workflow that anyone can run on free Sentinel-2 imagery to generate habitat predictions from scratch with only a handful of hand-labelled pixels.

<p align="right">(<a href="#readme-top">back to top</a>)</p>

## Background

Shallow reef zones are among the most biodiverse ecosystems on Earth, supporting nearly 25% of all marine species. These structures play vital roles in coastal protection, fisheries, and global biodiversity. However, they are increasingly under threat from climate change, coral bleaching, pollution, and destructive human activity.

Accurately classifying these features using satellite imagery and machine learning allows us to monitor reef health at scale, detect early signs of degradation, and support conservation planning. This is essential for protecting ecosystems that are both ecologically rich and critically endangered.

Traditional benthic surveys cannot keep pace across thousands of dispersed reefs, and global mapping products like the Allen Coral Atlas, although invaluable, are computationally heavy and rely on extensive training data that may not exist for every site. Because our pipeline needs just two co-registered tiles, the method can be ported along a reef tract in hours, supporting near-real-time monitoring after storms or heatwaves.

<p align="right">(<a href="#readme-top">back to top</a>)</p>

## Sentinel-2 

### Mission at a glance  
Sentinel-2 is part of ESA’s Copernicus fleet and comprises the twin satellites Sentinel-2A (2015) and Sentinel-2B (2017).  
They share a sun-synchronous 786 km orbit, phased 180 ° apart, giving:

| Parameter | Value | Why it matters for reefs |
|-----------|-------|--------------------------|
| **Revisit time** | 10 days per sat, 5 days combined (equator) | Frequent looks capture bleaching events and seasonal change |
| **Swath width** | 290 km | One pass covers entire archipelagos |

The Sentinel-2 Multi-Spectral Instrument (MSI) is a push-broom camera: as the satellite advances, a single linear detector sweeps out a continuous image strip. Incoming light is folded by three mirrors and split by a beam-splitter onto two focal-plane assemblies (FPAs), one covering the visible to near-infra-red range and the other the short-wave infra-red. Across these FPAs, 13 strip filters carve out the discrete spectral bands used by Sentinel-2: four bands are captured at 10 m ground resolution (blue, green, red, NIR), six at 20 m, and three at 60 m.

![Sentinel-2 multi-band imaging overview](https://github.com/JNathan18/Banner/blob/main/image.png)

### Why only Blue, Green, Red & NIR for this project  
1. All bands are at 10 m which keeps spatial detail consistent.  
2. Blue/Green penetrate the water column; Red enhances benthic discrimination. 
3. Just four channels means smaller tensors which means faster training & real-time inference on edge devices.
4. NIR enables computation of NDWI masks for our regions.

<p align="right">(<a href="#readme-top">back to top</a>)</p>

## Convolutional Neural Networks (CNNs)

### What are they?

A Convolutional Neural Network is the deep-learning work-horse for image recognition. Conceptually, it mimics the way our visual cortex processes information: small receptive fields scan across the scene, detect primitive patterns (edges, corners, colour blobs), and pass progressively richer abstractions to later layers that decide what the image contains.

**How it works**

1. **Feature-extraction stage:** Every convolutional layer slides a tiny filter (11 × 11 pixels in our case) over the input, multiplying and summing to produce a feature map. Non-linear activations (ReLU) follow, and an occasional pooling step downsamples the map, keeping only the most salient responses.  Because the same weights are reused everywhere (weight sharing), the network is compact and learns position-invariant patterns.

2. **Classification stage:** Once enough feature maps have been stacked, the 3-D tensor is flattened and fed to one or more fully-connected layers.  These dense neurons mix the extracted cues and output a probability for each class via a soft-max function.

<div align="center">

![Convolutional Neural Network architecture](https://github.com/JNathan18/Banner/blob/main/Sen_2_Infographic_cnn.png)

</div>

### Why we chose a CNN for reef mapping

Convolutions inspect only a handful of pixels at a time, perfect for teasing apart benthic textures (live coral, algae, sand) that occupy just a few Sentinel-2 pixels. With only the four 10 m bands (Blue, Green, Red, NIR) as input, a shallow CNN compiles to < 1 MB when quantised.

<p align="right">(<a href="#readme-top">back to top</a>)</p>

## NDWI  
### What is it?
The Normalised Difference Water Index (NDWI) was introduced by McFeeters (1996) as a simple way to highlight open water using only two spectral bands:

$$
\text{NDWI} = \frac{\text{Green} - \text{NIR}}{\text{Green} + \text{NIR}}
$$

* **Sentinel-2 band mapping** — Green = B3 (560 nm); NIR = B8 (842 nm).  
* **Value range** — NDWI ∈ [-1, +1].  Positive values tend to mark water; negative values mark vegetation, bare land, or coral substrate.

### Why include NDWI in our pipeline?  
 
A lightweight CNN like ours may confuse deep ocean noise for genuine reef signals. By introducing an NDWI filter, we reduce spurious detections by masking out areas unlikely to contain water. The index is also invariant to overall brightness (ratio form), so it supplies contrast information that the four raw Sentinel-2 bands (Blue, Green, Red, NIR) do not capture on their own. This gives the CNN crucial context for distinguishing where ‘wet’ ends and ‘dry’ begins. On top of this it is lightweight on compute and can be performed as a one line operation on-the-fly during data loading.

<p align="right">(<a href="#readme-top">back to top</a>)</p>

## End-to-End Workflow at a Glance

1. **Select cloud-free Sentinel-2 tiles**  
   Query neighbouring tiles acquired on the same date to guarantee spectral consistency.

2. **Compute an NDWI filter**  
   An NDWI layer will also be added as a 5ᵗʰ channel to every 11 × 11 patch.

3. **Define Regions of Interest (ROIs)**  
   Draw polygons on one “reference” tile covering reef flat, shallow sand, etc.

4. **Label the ROIs with IRIS**  
   Intelligently Reinforced Image Segmentation (IRIS) software auto-classifies polygons and exports pixel-wise masks.

5. **Build a balanced training set**  
   Sample equal numbers of patches per class.

6. **Train the CNN**  
   Two-block ConvNet, Early-Stopping, ReduceLR; NDWI included as an extra input channel.

7. **Roll-out on the unseen tile**  
   Predict on a second, previously untouched tile; NDWI recomputed on-the-fly;  CodeCarbon logs CO₂, kWh, CPU/GPU power for the inference run.

8. **Evaluate & analyse**  
   This includes a Confusion matrix as well as f1-scores, precision, recall and accuracy for our classes, overlay of our classification against the ACA as the ground truth.

<p align="right">(<a href="#readme-top">back to top</a>)</p>

## Getting Started

### Prerequisites
* List software versions, libraries, or accounts needed  
  ```bash
  pip install -r requirements.txt


