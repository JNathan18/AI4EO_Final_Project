<!-- Improved compatibility of “Back to top” link -->
<a name="readme-top"></a>

<!-- PROJECT LOGO / BANNER -->
<!-- Replace the src URL or delete completely -->
<p align="center">
  <img src="https://github.com/JNathan18/Banner/blob/main/GBR_Logo2.jpg?raw=true" alt="title_banner">
</p>

<h1 align="center">Detection of Shallow Reef Structures Using Machine Learning</h1>
<p align="center">
  Training a CNN to classify shallow reef structures using SENTINEL-2 data
</p>
<br />

<!-- TABLE OF CONTENTS (clickable) -->
<details>
  <summary>Table of Contents</summary>
  <ol>
    <li>
      <a href="#about-the-project">About&nbsp;The&nbsp;Project</a>
      <ul>
        <li><a href="#background">Background</a></li>
        <li><a href="#key-concepts">Key Concepts</a></li>
        <li><a href="#methodology">Methodology</a></li>
      </ul>
    </li>
    <li>
      <a href="#getting-started">Getting Started</a>
      <ul>
        <li><a href="#prerequisites">Prerequisites</a></li>
        <li><a href="#datasets-or-inputs">Datasets&nbsp;or&nbsp;Inputs</a></li>
      </ul>
    </li>
    <li><a href="#usage">Usage</a></li>
    <li><a href="#license">License</a></li>
    <li><a href="#contact">Contact</a></li>
    <li>
      <a href="#acknowledgments">Acknowledgments</a>
      <ul>
        <li><a href="#references">References</a></li>
      </ul>
    </li>
  </ol>
</details>

<!-- ABOUT THE PROJECT -->
## About The Project

This project was developed as the final assignment for the GEOL0069 (AI for Earth Observation) module at University College London. The module equips students with the skills to apply machine learning (ML) techniques to the growing volume of data generated by Earth observation satellites.

In this project, we apply supervised learning to extract a range of shallow reef features from satellite imagery. We focus on data from Sentinel-2, chosen for its high-resolution multispectral capabilities; an essential requirement for our algorithm of choice, the Convolutional Neural Network (CNN). The aim is to demonstrate how modern ML tools can support ecological monitoring by providing a minimal-footprint workflow that anyone can run on free Sentinel-2 imagery to generate habitat predictions from scratch with only a handful of hand-labelled pixels.

### Background

Shallow reef zones are among the most biodiverse ecosystems on Earth, supporting nearly 25% of all marine species. These structures play vital roles in coastal protection, fisheries, and global biodiversity. However, they are increasingly under threat from climate change, coral bleaching, pollution, and destructive human activity.

Accurately classifying these features using satellite imagery and machine learning allows us to monitor reef health at scale, detect early signs of degradation, and support conservation planning. This is essential for protecting ecosystems that are both ecologically rich and critically endangered.

Traditional benthic surveys cannot keep pace across thousands of dispersed reefs, and global mapping products like the Allen Coral Atlas, although invaluable, are computationally heavy and rely on extensive training data that may not exist for every site. Because our pipeline needs just two co-registered tiles, the method can be ported along a reef tract in hours, supporting near-real-time monitoring after storms or heatwaves.

## Key Concepts

## Sentinel-2 — why we use it for lightweight reef classification  
![Sentinel-2 multi-band imaging overview](assets/sentinel2_overview.png)

### Mission at a glance  
Sentinel-2 is part of ESA’s **Copernicus** fleet and comprises the twin satellites **Sentinel-2A** (2015) and **Sentinel-2B** (2017).  
They share a sun-synchronous 786 km orbit, phased 180 ° apart, giving:

| Parameter | Value | Why it matters for reefs |
|-----------|-------|--------------------------|
| **Revisit time** | 10 days per sat, **5 days combined** (equator) | Frequent looks capture bleaching events and seasonal change |
| **Swath width** | 290 km | One pass covers entire archipelagos |
| **Local overpass** | ~10 : 30 a.m. LTAN | Consistent illumination → easier atmospheric correction |

### The Multi-Spectral Instrument (MSI)  
* Push-broom scanner (linear focal plane).  
* Three mirrors fold the light path, then a beam-splitter feeds **two focal-plane assemblies (FPAs)**:  
  * **VNIR FPA:** 0.433–0.948 µm  
  * **SWIR FPA:** 1.37–2.19 µm  
* **Strip filters** on each detector line create **13 discrete bands**; on-board diffuser + dark shutter handle calibration.

| FPA | Bands (λ<sub>c</sub> / nm) | Native GSD |
|-----|---------------------------|------------|
| **VNIR** | B1 443, **B2 490 (Blue)**, **B3 560 (Green)**, **B4 665 (Red)**, B5 705, B6 740, B7 783, **B8 842 (NIR)**, B8A 865, B9 945 | 10 m (B2–B4, B8) <br>20 m (B5–B9) |
| **SWIR** | B10 1375, B11 1610, B12 2190 | 60 m (B10) <br>20 m (B11–B12) |

### Why only Blue, Green, Red & NIR for this project  
1. **All at 10 m** → keeps spatial detail consistent.  
2. Blue/Green penetrate the water column; Red enhances benthic discrimination; NIR gives razor-sharp land/water masks.  
3. Four channels = smaller tensors → faster training & real-time inference on edge devices.

### Methodology
[Describe your overall approach or algorithm.  
Bullet points or sub-headings (“Data Pre-processing”, “Model”, “Evaluation”) often read well here.]

<p align="right">(<a href="#readme-top">back to top</a>)</p>

## Getting Started

### Prerequisites
* List software versions, libraries, or accounts needed  
  ```bash
  pip install -r requirements.txt


